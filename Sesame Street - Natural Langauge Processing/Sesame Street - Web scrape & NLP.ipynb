{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import pandas as pd \n",
    "import os \n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import csv\n",
    "from pyquery import PyQuery as pq\n",
    "from lxml import etree\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping MetroLyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sesamesongs = [] \n",
    "\n",
    "for x in range(1,11):\n",
    "    newsong = {'Sesame Street': 'http://www.metrolyrics.com/sesame-street-alpage-' + str(x) + '.html'}\n",
    "    sesamesongs.append(newsong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Allsongs = []\n",
    "\n",
    "for l in sesamesongs:\n",
    "    for key, value in l.items():\n",
    "        \n",
    "        print(value) ## To keep track of progress\n",
    "        \n",
    "        response = requests.get(value)\n",
    "        doc = pq(response.content)       \n",
    "        titles = doc('.title')\n",
    "\n",
    "        \n",
    "        for title in titles:\n",
    "            if 'Sesame' in (title.attrib['title']):\n",
    "                header = (title.attrib['title'])\n",
    "                response_title = requests.get(title.attrib['href'])\n",
    "                doc2 = pq(response_title.content)     \n",
    "                verse = doc2('.verse')\n",
    "            \n",
    "                lyrics =  (verse.text())\n",
    "            \n",
    "                print(header) ## To keep track of progress\n",
    "            \n",
    "                newsong = {'Show' : key, 'Lyrics': lyrics, 'Song Name' : header } #### FIGURE THIS OUT!... Song Name' : header \n",
    "                Allsongs.append(newsong)\n",
    "            \n",
    "        \n",
    "                time.sleep(4.5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Allsongs.to_csv('SesameSongs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP - Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SesameSongs = pd.read_csv('SesameSongs.csv').drop('Unnamed: 0',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SesameSongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SesameSongs_clean = SesameSongs.apply(lambda x: x.str.strip()).replace('', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SesameSongs_clean = SesameSongs_clean.dropna().replace(r'\\([^)]*\\)','', regex=True).replace(r'\\#\\w*:','', regex=True).replace(r'\\w*:','', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SesameSongs_clean['Lyrics'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Column: Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SesameSongs_clean['Word Count'] = SesameSongs_clean['Lyrics'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### New Column: Non-English words removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "OnlyEng = []\n",
    "\n",
    "for song in SesameSongs_clean['Lyrics']:\n",
    "    sent = \" \".join(w for w in nltk.wordpunct_tokenize(song) \\\n",
    "                    if w.lower() in words or not w.isalpha())\n",
    "    OnlyEng.append(sent)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SesameSongs_clean['Only English Words'] = OnlyEng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### New Column: Number of unique English words used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Uniquecount = []\n",
    "\n",
    "for song in SesameSongs_clean['Only English Words']:\n",
    "    uniqueWords = len(set(nltk.wordpunct_tokenize(song)))\n",
    "    \n",
    "    Uniquecount.append(uniqueWords)\n",
    "\n",
    "# uniqueWords = list(set(\" \".join(SesameSongs_clean['Only English Words']).lower().split(\" \")))\n",
    "\n",
    "SesameSongs_clean['Unique Eng Words'] = Uniquecount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### New Column: Proportion of total words that are unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Uniqueprop = []\n",
    "\n",
    "for song in SesameSongs_clean['Only English Words']:\n",
    "    unique = len(set(nltk.wordpunct_tokenize(song)))\n",
    "    prop = round(unique / len(nltk.wordpunct_tokenize(song)),3)\n",
    "    \n",
    "    Uniqueprop.append(prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SesameSongs_clean['Prop of unique words'] = Uniqueprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### New Column: Count of how many numbers are mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "numlist = ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','one','two','three','four','five','six','seven','eight','nine','ten']\n",
    "\n",
    "Numcount = []\n",
    "counter = 0\n",
    "\n",
    "for song in SesameSongs_clean['Lyrics']:\n",
    "    counter = 0\n",
    "    for w in nltk.wordpunct_tokenize(song):\n",
    "        if w in numlist:\n",
    "            counter += 1\n",
    "    \n",
    "    Numcount.append(counter)\n",
    "    \n",
    "\n",
    "SesameSongs_clean['How many numbers?'] = Numcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### New Column: Proportion of song that starts with the letter that appears most often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from string import ascii_lowercase\n",
    "\n",
    "nonnumeric = ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','.',',','!','?','...','#','\"',\"'\",')','-']\n",
    "\n",
    "listodic = []\n",
    "letterprop = []\n",
    "\n",
    "d = {}\n",
    "\n",
    "for song in SesameSongs_clean['Lyrics']:\n",
    "    d = {}\n",
    "    x = nltk.wordpunct_tokenize(song)\n",
    "    for word in x:\n",
    "        word = word.lower()\n",
    "        if word not in nonnumeric:\n",
    "            if word[0] not in d:\n",
    "                d[word[0]] = 1\n",
    "            \n",
    "            else:\n",
    "                d[word[0]] += 1\n",
    "                  \n",
    "    v=list(d.values())\n",
    "    most = (max(v))\n",
    "    \n",
    "    letterprop.append(round(most/len(nltk.wordpunct_tokenize(song)),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SesameSongs_clean['Letter Proportion'] = letterprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Column: Number of times \"you\" \"yours \"your\" is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive = ['you','your',\"y'all\",'you all','our','we']\n",
    "rest = ['I','he','she','their','they','them','hers','his','theirs']\n",
    "\n",
    "subjects = []\n",
    "\n",
    "\n",
    "youcounter = 0\n",
    "restcounter = 0\n",
    "\n",
    "\n",
    "for song in SesameSongs_clean['Only English Words']:\n",
    "    youcounter = 0\n",
    "    restcounter = 0\n",
    "    for w in nltk.wordpunct_tokenize(song):\n",
    "        if w in interactive:\n",
    "            youcounter += 1\n",
    "        if w in rest:\n",
    "            restcounter +=1\n",
    "    \n",
    "    if (restcounter+youcounter) != 0:\n",
    "        youprop = (youcounter/(youcounter+restcounter))\n",
    "        \n",
    "    else:\n",
    "        youprop = 0\n",
    "    \n",
    "    subjects.append(youprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SesameSongs_clean['Interactive language prop'] = subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### New Columns: Polarity and Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SesameSongs_clean = SesameSongs_clean.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "subjectivity = []\n",
    "polarity = []\n",
    "\n",
    "for word in range(len(SesameSongs_clean['Lyrics'])):\n",
    "    w = str(SesameSongs_clean[\"Lyrics\"][word])\n",
    "    d = list(TextBlob(w).sentiment)\n",
    "    polarity.append(d[0])\n",
    "    subjectivity.append(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SesameSongs_clean['Polarity'] = polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SesameSongs_clean['Subjectivity'] = subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### New Column: Number of words outside Baladan word list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Creating columns to identify how many \"fringe\" words are used (i.e. words that are less common in adult speech).** <br>\n",
    "Baladan 347 words account for 75% of adult speech. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### **Creating Baladan List**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "baladinpd = pd.read_csv('Baladin_WordList.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "baladin = []\n",
    "\n",
    "import csv\n",
    "with open('Baladin_WordList.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    baladinlist = list(reader)\n",
    "\n",
    "for listy in baladinlist:\n",
    "    for word in listy:\n",
    "        if word != '':\n",
    "            baladin.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(baladin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### **Counting number of words outside of the list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fringewords = []\n",
    "\n",
    "fringecounts = []\n",
    "fringeprop = []\n",
    "    \n",
    "\n",
    "for song in SesameSongs_clean['Only English Words']:\n",
    "    fringewords = []\n",
    "    for w in nltk.wordpunct_tokenize(song):\n",
    "        w = w.lower()\n",
    "        if w not in baladin:\n",
    "            if w not in nonnumeric:\n",
    "                fringewords.append(w)\n",
    "    \n",
    "    \n",
    "    fringey = (len(set(fringewords)))\n",
    "    uniques = len(set(nltk.wordpunct_tokenize(song)))\n",
    "    \n",
    "    fringecounts.append(fringey)   \n",
    "    fringeprop.append(round((fringey/uniques), 3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SesameSongs_clean['Adult fringe count'] = fringecounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SesameSongs_clean['Adult fringe prop'] = fringeprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Column: Number of words outside Marvin word list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating columns to identify how many \"fringe\" words are used (i.e. words that are less in toddler speech).** <br>\n",
    "Baladan 333 words account for majority of pre-schooler core speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### **Creating Marvin list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "marvinpd = pd.read_csv('Marvin_WordList.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "marvin = []\n",
    "\n",
    "import csv\n",
    "with open('Marvin_WordList.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    marvinlist = list(reader)\n",
    "\n",
    "for listy in marvinlist:\n",
    "    for word in listy:\n",
    "        if len(word)>0:\n",
    "            if word[0] not in numlist:\n",
    "                marvin.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "marvin = (set(marvin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Counting number of words outside of the list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfringewords = []\n",
    "\n",
    "mfringecounts = []\n",
    "mfringeprop = []\n",
    "    \n",
    "\n",
    "for song in SesameSongs_clean['Only English Words']:\n",
    "    mfringewords = []\n",
    "    for w in nltk.wordpunct_tokenize(song):\n",
    "        w = w.lower()\n",
    "        if w not in marvin:\n",
    "            if w not in nonnumeric:\n",
    "                mfringewords.append(w)\n",
    "    \n",
    "    \n",
    "    fringey = (len(set(mfringewords)))\n",
    "    uniques = len(set(nltk.wordpunct_tokenize(song)))\n",
    "    \n",
    "    mfringecounts.append(fringey)   \n",
    "    mfringeprop.append(round((fringey/uniques), 3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SesameSongs_clean['PreK fringe count'] = mfringecounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SesameSongs_clean['PreK fringe prop'] = mfringeprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SesameSongs_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "** Selecting inputs for the model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "S_inputs = SesameSongs_clean.filter(items = [\n",
    "#        'Word Count', \n",
    "#        'Unique Eng Words',\n",
    "       'Prop of unique words',  \n",
    "#     'How many numbers?',\n",
    "#     'Letter Proportion',\n",
    "    'Interactive language prop',\n",
    "    'Adult fringe prop',\n",
    "    'PreK fringe prop',\n",
    "#     'PreK fringe count',\n",
    "#     'Adult fringe count'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "** Scaling **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "S_inputs['Word Count'] = scale(S_inputs['Word Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "S_inputs['How many numbers?'] = scale(S_inputs['How many numbers?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S_inputs['PreK fringe count'] = scale(S_inputs['PreK fringe count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "S_inputs['Adult fringe count'] = scale(S_inputs['Adult fringe count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Running Hierarchical Aggllomerative Clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Aclus = AgglomerativeClustering(n_clusters = 3,linkage = 'ward')\n",
    "Aclus.fit(S_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p = Aclus.fit_predict(S_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# newsong = np.array([0.28,0.8,0.34,0.5])\n",
    "# newsong2 = np.reshape(newsong,(1,-1))\n",
    "\n",
    "# Aclus.predict(S_inputs.iloc[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Aclus.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SesameSongs_clustered = SesameSongs_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SesameSongs_clustered['Cluster'] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SesameSongs_clustered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### EDA on each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SesameSongs_clustered['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cluster_analysis = SesameSongs_clustered.groupby(['Cluster'])[\n",
    "    'Word Count', \n",
    "    'Unique Eng Words',\n",
    "    'Prop of unique words',  \n",
    "    'How many numbers?',\n",
    "    'Letter Proportion',\n",
    "    'Interactive language prop',\n",
    "    'Adult fringe prop',\n",
    "    'PreK fringe prop',\n",
    "    'PreK fringe count',\n",
    "    'Adult fringe count'\n",
    "].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cluster_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**It seems like generally... ** <br>\n",
    "0. Has the highest proportion of new words.\n",
    "1. Have the most words. \n",
    "2. Has the highest proportion of words that break the fourth wall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LDA on each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "** Seperating each cluster into own dataframe **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cluster0 = SesameSongs_clustered.loc[SesameSongs_clustered['Cluster'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cluster1 = SesameSongs_clustered.loc[SesameSongs_clustered['Cluster'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cluster2 = SesameSongs_clustered.loc[SesameSongs_clustered['Cluster'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "** LDA on each cluster **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Cluster 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "# sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "# logging for gensim (set to INFO)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Songs0 = cluster0['Only English Words'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1), stop_words=\"english\")\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(Songs0)\n",
    "\n",
    "x = vectorizer.fit_transform(Songs0)\n",
    "# # then, use `get_feature_names` to return the tokens\n",
    "# print(vectorizer.get_feature_names())\n",
    "\n",
    "counts = vectorizer.transform(Songs0).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components= 3, max_iter = 15, random_state=0, \n",
    "                                n_jobs=-1, learning_method='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "topics = lda.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda, x, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "Songs0all = []\n",
    "\n",
    "for song in Songs0:\n",
    "    for word in song:\n",
    "        Songs0all.append(word)\n",
    "    \n",
    "    \n",
    "pop0words = nltk.probability.FreqDist(Songs0)\n",
    "\n",
    "print(pop0words)\n",
    "\n",
    "v=list(pop0words.values())\n",
    "sort = (sorted(v))\n",
    "\n",
    "from operator import itemgetter\n",
    "# listy = sorted(pop0words.items(), key=itemgetter(reverse=True))\n",
    "\n",
    "# listy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Cluster1 = Cluster1.replace('.', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Songs1 = cluster1['Only English Words'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Songs1 = filter(lambda i:not(type(i) is str), Songs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,1), stop_words=\"english\")\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(Songs1)\n",
    "\n",
    "x = vectorizer.fit_transform(Songs1)\n",
    "# # then, use `get_feature_names` to return the tokens\n",
    "# print(vectorizer.get_feature_names())\n",
    "\n",
    "counts = vectorizer.transform(Songs1).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Songs2 = cluster2['Only English Words'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,1), stop_words=\"english\")\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(Songs2)\n",
    "\n",
    "x = vectorizer.fit_transform(Songs2)\n",
    "# # then, use `get_feature_names` to return the tokens\n",
    "# print(vectorizer.get_feature_names())\n",
    "\n",
    "counts = vectorizer.transform(Songs2).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components= 3, max_iter = 15, random_state=0, \n",
    "                                n_jobs=-1, learning_method='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "topics = lda.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda, x, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Song writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mysong = 'When Im feeling mad or sad I Follow my imagination till things arent so bad Try these things I like to do, And before you know it youll be joyful too. I Listen to the sounds of a baby bird Or Grab a book and read every word I Bop my head to a song’s little rhymes And sing along to pass the time I think of a place Id like to visit, And in my mind, its quite exquisite! I Honk the horn of an blue car I’m nearly there, I’m not so far! I Follow my dog to his favorite park, And befriend a furry monster when it gets dark, I Count the Hairs on the head of a dear pied piper Or write happy words on a typewriter! These are the things I like to do, I think youll love to do them too!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "** Baladin score **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fringewords = []\n",
    "\n",
    "fringecounts = []\n",
    "fringeprop = []\n",
    "    \n",
    "for w in nltk.wordpunct_tokenize(mysong):\n",
    "    w = w.lower()\n",
    "    if w not in baladin:\n",
    "        if w not in nonnumeric:\n",
    "            fringewords.append(w)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "wordlist = fringewords.remove('youll')\n",
    "wordlist = fringewords.remove('arent')\n",
    "\n",
    "fringey = (len(set(fringewords)))\n",
    "uniques = len(set(nltk.wordpunct_tokenize(mysong)))\n",
    "    \n",
    "fringecounts.append(fringey)   \n",
    "fringeprop.append(round((fringey/uniques), 3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fringeprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fringecounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "balwordlist = set(fringewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "balwordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "** Marvin score **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mfringewords = []\n",
    "\n",
    "mfringecounts = []\n",
    "mfringeprop = []\n",
    "    \n",
    "\n",
    "\n",
    "for w in nltk.wordpunct_tokenize(mysong):\n",
    "    w = w.lower()\n",
    "    if w not in marvin:\n",
    "        if w not in nonnumeric:\n",
    "            mfringewords.append(w)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fringey = (len(set(mfringewords)))\n",
    "uniques = len(set(nltk.wordpunct_tokenize(mysong)))\n",
    "    \n",
    "mfringecounts.append(fringey)   \n",
    "mfringeprop.append(round((fringey/uniques), 3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(mfringewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mfringecounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mfringeprop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
